# NLG-papers
Some papers in NLG studied during my master

1. [Graph Pre-training for AMR Parsing and Generation](https://aclanthology.org/2022.acl-long.415/)
2. [A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models](https://aclanthology.org/2022.acl-long.197/)
3. [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://aclanthology.org/2022.acl-long.576/)
4. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
5. [Prix-LM: Pretraining for Multilingual Knowledge Base Construction](https://aclanthology.org/2022.acl-long.371.pdf)
6. [Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation](https://aclanthology.org/2022.acl-long.61.pdf)
7. [Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation](https://aclanthology.org/2022.acl-long.185.pdf)
8. [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://aclanthology.org/2022.acl-long.185.pdf)
9. [mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer](https://aclanthology.org/2021.naacl-main.41/)
10. [Neural Pipeline for Zero-Shot Data-to-Text Generation](https://aclanthology.org/2022.acl-long.271/)
11. [Improving Compositional Generalization with Self-Training for Data-to-Text Generation](https://aclanthology.org/2022.acl-long.289.pdf)
12. [WIKITABLET: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections](https://aclanthology.org/2021.findings-acl.17/)
13. [TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching](https://aclanthology.org/2020.coling-main.179/)
